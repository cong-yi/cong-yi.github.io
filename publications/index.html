<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="oOo9ddi_Xn45yjThIP9KrMf2w9vg72HQkBPbkjnZtCE"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Congyi Zhang</title> <meta name="author" content="Congyi Zhang"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="computer graphics, virtual reality"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icon.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://cong-yi.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://cong-yi.github.io/"><span class="font-weight-bold">Congyi</span> Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2023</h2> <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">ICCV 2023</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/dualmeshudf.png" width="300"></p> </div> <div class="col"> <div class="title">Surface Extraction from Neural Unsigned Distance Fields</div> <div class="author"> <span class="bolded">Congyi Zhang*</span>, <a href="https://carrie-lin.github.io/" target="_blank" rel="noopener noreferrer">Guying Lin*</a>, Lei Yang, <a href="https://people.tamu.edu/~xinli/" target="_blank" rel="noopener noreferrer">Xin Li</a>, <a href="https://i.cs.hku.hk/~taku/" target="_blank" rel="noopener noreferrer">Taku Komura</a>, <a href="https://people.engr.tamu.edu/schaefer/index.html" target="_blank" rel="noopener noreferrer">Scott Schaefer</a>, <a href="https://people.engr.tamu.edu/keyser/index.html" target="_blank" rel="noopener noreferrer">John Keyser</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> (* equal contribution)</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08878" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://cong-yi.github.io/projects/dualmeshudf/" class="btn btn-sm z-depth-0" role="button">Project</a> <a href="https://github.com/cong-yi/DualMesh-UDF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>We propose a method, named DualMesh-UDF, to extract a surface from unsigned distance functions (UDFs), encoded by neural networks, or neural UDFs. Neural UDFs are becoming increasingly popular for surface representation because of their versatility in presenting surfaces with arbitrary topologies, as opposed to the signed distance function that is limited to representing a closed surface. However, the applications of neural UDFs are hindered by the notorious difficulty in extracting the target surfaces they represent. Recent methods for surface extraction from a neural UDF suffer from significant geometric errors or topological artifacts due to two main difficulties: (1) A UDF does not exhibit sign changes; and (2) A neural UDF typically has substantial approximation errors. DualMesh-UDF addresses these two difficulties. Specifically, given a neural UDF encoding a target surface S to be recovered, we first estimate the tangent planes of S at a set of sample points close to S. Next, we organize these sample points into local clusters, and for each local cluster, solve a linear least squares problem to determine a final surface point. These surface points are then connected to create the output mesh surface, which approximates the target surface. The robust estimation of the tangent planes of the target surface and the subsequent minimization problem constitute our core strategy, which contributes to the favorable performance of DualMesh-UDF over other competing methods. To efficiently implement this strategy, we employ an adaptive Octree. Within this framework, we estimate the location of a surface point in each of the octree cells identified as containing part of the target surface. Extensive experiments show that our method outperforms existing methods in terms of surface reconstruction quality while maintaining comparable computational efficiency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">ICCV 2023</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/sherd.jpg" width="300"></p> </div> <div class="col"> <div class="title">Batch-based Model Registration for Fast 3D Sherd Reconstruction</div> <div class="author"> <a href="https://jiepengwang.github.io/" target="_blank" rel="noopener noreferrer">Jiepeng Wang</a>,  <span class="bolded">Congyi Zhang</span>, <a href="https://totoro97.github.io/" target="_blank" rel="noopener noreferrer">Peng Wang</a>, <a href="https://people.tamu.edu/~xinli/" target="_blank" rel="noopener noreferrer">Xin Li</a>, Peter J. Cobb, <a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.06897" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://jiepengwang.github.io/FIRES/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> <a href="https://github.com/jiepengwang/FIRES" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>3D reconstruction techniques have widely been used for digital documentation of archaeological fragments. However, efficient digital capture of fragments remains as a challenge. In this work, we aim to develop a portable, high-throughput, and accurate reconstruction system for efficient digitization of fragments excavated in archaeological sites. To realize high-throughput digitization of large numbers of objects, an effective strategy is to perform scanning and reconstruction in batches. However, effective batch-based scanning and reconstruction face two key challenges: 1) how to correlate partial scans of the same object from multiple batch scans, and 2) how to register and reconstruct complete models from partial scans that exhibit only small overlaps. To tackle these two challenges, we develop a new batch-based matching algorithm that pairs the front and back sides of the fragments, and a new Bilateral Boundary ICP algorithm that can register partial scans sharing very narrow overlapping regions. Extensive validation in labs and testing in excavation sites demonstrate that these designs enable efficient batch-based scanning for fragments. We show that such a batch-based scanning and reconstruction pipeline can have immediate applications on digitizing sherds in archaeological excavations.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">SIGGRAPH ASIA 2022</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/dmm.png" width="300"></p> </div> <div class="col"> <div class="title">An Implicit Parametric Morphable Dental Model</div> <div class="author"> <span class="bolded">Congyi Zhang</span>, <a href="https://people.mpi-inf.mpg.de/~elgharib/" target="_blank" rel="noopener noreferrer">Mohamed Elgharib</a>, <a href="https://people.mpi-inf.mpg.de/~gfox/" target="_blank" rel="noopener noreferrer">Gereon Fox</a>, Min Gu, <a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.11402" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://dl.acm.org/doi/10.1145/3550454.3555469" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://vcai.mpi-inf.mpg.de/projects/DMM/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> <a href="https://github.com/cong-yi/DMM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>3D Morphable models of the human body capture variations among subjects and are useful in reconstruction and editing applications. Current dental models use an explicit mesh scene representation and model only the teeth, ignoring the gum. In this work, we present the first parametric 3D morphable dental model for both teeth and gum. Our model uses an implicit scene representation and is learned from rigidly aligned scans. It is based on a component-wise representation for each tooth and the gum, together with a learnable latent code for each of such components. It also learns a template shape thus enabling several applications such as segmentation, interpolation and tooth replacement. Our reconstruction quality is on par with the most advanced global implicit representations while enabling novel applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">TVCG</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/creatureshop.jpg" width="300"></p> </div> <div class="col"> <div class="title">CreatureShop: Interactive 3D Character Modeling and Texturing from a Single Color Drawing</div> <div class="author"> <span class="bolded">Congyi Zhang</span>, Lei Yang, Nenglun Chen, <a href="https://www.cs.ubc.ca/~nvining/" target="_blank" rel="noopener noreferrer">Nicholas Vining</a>, <a href="https://www.cs.ubc.ca/~sheffa/" target="_blank" rel="noopener noreferrer">Alla Sheffer</a>, <a href="https://i.cs.hku.hk/~fcmlau/" target="_blank" rel="noopener noreferrer">Francis C.M. Lau</a>, Guoping Wang, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.05572" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://cong-yi.github.io/projects/creatureshop/" class="btn btn-sm z-depth-0" role="button">Project</a> </div> <div class="abstract hidden"> <p>Creating 3D shapes from 2D drawings is an important problem with applications in content creation for computer animation and virtual reality. We introduce a new sketch-based system, CreatureShop, that enables amateurs to create high-quality textured 3D character models from 2D drawings with ease and efficiency. CreatureShop takes an input bitmap drawing of a character (such as an animal or other creature), depicted from an arbitrary descriptive pose and viewpoint, and creates a 3D shape with plausible geometric details and textures from a small number of user annotations on the 2D drawing. Our key contributions are a novel oblique view modeling method, a set of systematic approaches for producing plausible textures on the invisible or occluded parts of the 3D character (as viewed from the direction of the input drawing), and a user-friendly interactive system. We validate our system and methods by creating numerous 3D characters from various drawings, and compare our results with related works to show the advantages of our method. We perform a user study to evaluate the usability of our system, which demonstrates that our system is a practical and efficient approach to create fully-textured 3D character models for novice users.</p> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">CHI 2021</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/handpainter.jpg" width="300"></p> </div> <div class="col"> <div class="title">HandPainter - 3D Sketching in VR with Hand-Based Physical Proxy</div> <div class="author"> <a href="https://yingjiang96.github.io/" target="_blank" rel="noopener noreferrer">Ying Jiang</a>,  <span class="bolded">Congyi Zhang<sup>†</sup></span>, <a href="http://sweb.cityu.edu.hk/hongbofu/" target="_blank" rel="noopener noreferrer">Hongbo Fu</a>, Alberto Cannavò, <a href="https://staff.polito.it/fabrizio.lamberti/" target="_blank" rel="noopener noreferrer">Fabrizio Lamberti</a>, Henry Y K Lau, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> († corresponding author)</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445302" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://yingjiang96.github.io/handpainter" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> </div> <div class="abstract hidden"> <p>3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index finger of the other hand as a 3D pen. To this end, we first perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the findings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the effectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"></div> <div class="col"> <div class="title">Anthropometric accuracy of three-dimensional average faces compared to conventional facial measurements</div> <div class="author">Zhiyi Shan, Richard Tai-Chiu Hsung,  <span class="bolded">Congyi Zhang</span>, Juanjuan Ji, Wing Shan Choi, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a>, Yanqi Yang, Min Gu, and Balvinder S Khambay</div> <div class="periodical"> <em>Scientific Reports</em>, 2021 </div> <div class="links"> <a href="https://www.nature.com/articles/s41598-021-91579-4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"><abbr class="badge">CVM 2021</abbr></div> <div class="col"> <div class="title">Homography-guided stereo matching for wide-baseline image interpolation</div> <div class="author">Yuan Chang,  <span class="bolded">Congyi Zhang</span>, Yisong Chen, and Guoping Wang</div> <div class="periodical"> <em>Computational Visual Media</em>, 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s41095-021-0225-9" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Image interpolation has a wide range of applications such as frame rate-up conversion and free viewpoint TV. Despite significant progresses, it remains an open challenge especially for image pairs with large displacements. In this paper, we first propose a novel optimization algorithm for motion estimation, which combines the advantages of both global optimization and a local parametric transformation model. We perform optimization over dynamic label sets, which are modified after each iteration using the prior of piecewise consistency to avoid local minima. Then we apply it to an image interpolation framework including occlusion handling and intermediate image interpolation. We validate the performance of our algorithm experimentally, and show that our approach achieves state-of-the-art performance.</p> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">IEEE VR 2020</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/dgaze.jpg" width="300"></p> </div> <div class="col"> <div class="title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</div> <div class="author"> <a href="https://cranehzm.github.io/" target="_blank" rel="noopener noreferrer">Zhiming Hu</a>, Sheng Li,  <span class="bolded">Congyi Zhang</span>, Kangrui Yi, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank" rel="noopener noreferrer">Dinesh Manocha</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8998375" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://cranehzm.github.io/DGaze.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> </div> <div class="abstract hidden"> <p>We conduct novel analyses of users’ gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. We first collect 43 users’ eye tracking data in 5 dynamic scenes under free-viewing conditions. Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users’ gaze positions. Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users’ gaze positions. Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. We further analyze our CNN architecture and verify the effectiveness of each component in our model. We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">SIGGRAPH ASIA 2020</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/cppm.jpg" width="300"></p> </div> <div class="col"> <div class="title">CPPM: Chi-Squared Progressive Photon Mapping</div> <div class="author">Zehui Lin, Sheng Li, Xinlu Zeng,  <span class="bolded">Congyi Zhang</span>, Jinzhu Jia, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank" rel="noopener noreferrer">Dinesh Manocha</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3414685.3417822" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://bactlink.github.io/CPPM.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> </div> <div class="abstract hidden"> <p>We present a novel chi-squared progressive photon mapping algorithm (CPPM) that constructs an estimator by controlling the bandwidth to obtain superior image quality. Our estimator has parametric statistical advantages over prior nonparametric methods. First, we show that when a probability density function of the photon distribution is subject to uniform distribution, the radiance estimation is unbiased under certain assumptions. Next, the local photon distribution is evaluated via a chi-squared test to determine whether the photons follow the hypothesized distribution (uniform distribution) or not. If the statistical test deems that the photons inside the bandwidth are uniformly distributed, bandwidth reduction should be suspended. Finally, we present a pipeline with a bandwidth retention and conditional reduction scheme according to the test results. This pipeline not only accumulates sufficient photons for a reliable chi-squared test, but also guarantees that the estimate converges to the correct solution under our assumptions. We evaluate our method on various benchmarks and observe significant improvement in the running time and rendering quality in terms of mean squared error over prior progressive photon mapping methods.</p> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <br> <br> <ol class="bibliography"> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">SMI 2019</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/deform.jpg" width="300"></p> </div> <div class="col"> <div class="title">Real-time editing of man-made mesh models under geometric constraints</div> <div class="author"> <span class="bolded">Congyi Zhang</span>, Lei Yang, Liyou Xu, Guoping Wang, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Wenping Wang</a> </div> <div class="periodical"> <em>Computers &amp; Graphics</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0097849319300901" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Editing man-made mesh models under multiple geometric constraints is a crucial need for product design to facilitate design exploration and iterative optimization. However, the presence of multiple geometric constraints (e.g. the radius of a cylindrical shape, distance from a point to a plane) as well as the high dimensionality of the discrete mesh representation of man-made models make it difficult to solve this constraint system in real-time. In this paper, we propose an approach based on subspace decomposition to achieve this goal. When a set of variables are edited by the user, the proposed method minimizes the residual of the constraint system in a least square sense to derive a new shape. The resulting shape shall comply with the assigned (extrinsic) constraints while maintaining the original (intrinsic) constraints analyzed from the given mesh model. In particular, we extract a meaningful subspace of the entire solution space based on the user’s edits to reduce the order of the problem, and solve the constraint system globally in real-time. Finally, we project the approximate solution back to the original solution space to obtain the editing result.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col abbr h-auto"> <abbr class="badge">IEEE VR 2019</abbr><p><img class="rounded" src="https://cong-yi.github.io/assets/img/teasers/sgaze.jpg" width="300"></p> </div> <div class="col"> <div class="title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</div> <div class="author"> <a href="https://cranehzm.github.io/" target="_blank" rel="noopener noreferrer">Zhiming Hu</a>,  <span class="bolded">Congyi Zhang</span>, Sheng Li, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank" rel="noopener noreferrer">Dinesh Manocha</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8643434" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://cranehzm.github.io/SGaze.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project</a> </div> <div class="abstract hidden"> <p>We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Congyi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: September 30, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>