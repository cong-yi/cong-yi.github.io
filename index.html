<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="oOo9ddi_Xn45yjThIP9KrMf2w9vg72HQkBPbkjnZtCE"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Congyi Zhang </title> <meta name="author" content="Congyi Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="computer graphics, virtual reality"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.jpg?8aba54abf2d5abc754923eac6f5dfda7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cong-yi.github.io/"> <script src="/assets/js/theme.js?f2531f05c6f8e1622518f4f5a1e385b1"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="https://dblp.org/pid/192/2791.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="mailto:%63%6F%6E%67%79%69%7A@%63%73.%75%62%63.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/cong-yi" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/congyi-zhang-391790135" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=-Ke6AGEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Congyi</span> Zhang </h1> <p class="desc">Postdoctoral Research and Teaching Fellow · The University of British Columbia</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?8ecc8066990bd46befdf3cbd1ba5de08" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a postdoctoral research and teaching fellow in the <a href="https://www.cs.ubc.ca/" rel="external nofollow noopener" target="_blank">Department of Computer Science at UBC</a> with Prof. <a href="https://www.cs.ubc.ca/~sheffa/" rel="external nofollow noopener" target="_blank">Alla Sheffer</a>. Prior to this position, I served as a research associate at <a href="https://www.cs.hku.hk/" rel="external nofollow noopener" target="_blank">HKU CS</a> working with Prof. <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a>. I also spent a year as a visiting researcher at <a href="https://www.mpi-inf.mpg.de/home" rel="external nofollow noopener" target="_blank">the Max Planck Institute for Informatics</a> with Prof. <a href="https://people.mpi-inf.mpg.de/~theobalt/" rel="external nofollow noopener" target="_blank">Christian Theobalt</a>. I earned my Ph.D. in computer science from <a href="https://eecs.pku.edu.cn/en/" rel="external nofollow noopener" target="_blank">Peking University</a> and my B.Sc. in mathematics from <a href="https://math.fudan.edu.cn/mathen/main.htm" rel="external nofollow noopener" target="_blank">Fudan University</a>. My recent research topics are focused on neural shape representation, AI for 3D content generation, human–computer interaction, and 3D reconstruction.</p> <p><strong>I am an incoming Assistant Professor in the <a href="https://cs.utdallas.edu/" rel="external nofollow noopener" target="_blank">Computer Science Department at UT Dallas</a> and am currently looking for self-motivated Ph.D. students starting in Spring/Fall 2026. If you’re interested in working with me, please feel free to send me your CV.</strong></p> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> For a complete and always up-to-date list, please check out my <a href="https://scholar.google.com/citations?user=-Ke6AGEAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> page. <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TOG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/nesi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nesi.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="NESI2025" class="col-sm-8"> <div class="title">NESI: Neural Explicit-Shape-Intersection-Based Geometry Representation</div> <div class="author"> <b>Congyi Zhang</b>, Jinfan Yang, <a href="https://ehedlin.github.io/" rel="external nofollow noopener" target="_blank">Eric Hedlin</a>, <a href="https://stakikawa.github.io/" rel="external nofollow noopener" target="_blank">Suzuran Takikawa</a>, <a href="https://www.cs.ubc.ca/~nvining/" rel="external nofollow noopener" target="_blank">Nicholas Vining</a>, <a href="https://www.cs.ubc.ca/~kmyi/" rel="external nofollow noopener" target="_blank">Kwang Moo Yi</a>, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a>, and <a href="https://www.cs.ubc.ca/~sheffa/" rel="external nofollow noopener" target="_blank">Alla Sheffer</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, Jul 2025 </div> <div class="periodical"> (will be presented at SIGGRAPH Asia 2025) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3742893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2409.06030" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.cs.ubc.ca/labs/imager/tr/2025/nesi/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:7PzlFSSx8tAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Compressed representations of 3D shapes that are compact, accurate, and can be processed efficiently directly in compressed form, are extremely useful for digital media applications. Recent approaches in this space focus on learned implicit or parametric representations. While implicits are well suited for tasks such as in-out queries, they lack natural 2D parameterization, complicating tasks such as texture or normal mapping. Conversely, parametric representations support the latter tasks but are ill-suited for occupancy queries. We propose a novel learned alternative to these approaches, based on intersections of localized explicit, or height-field, surfaces. Since explicits can be trivially expressed both implicitly and parametrically, NESI directly supports a wider range of processing operations than implicit alternatives, including occupancy queries and parametric access. We represent input shapes using a collection of differently oriented height-field bounded half-spaces combined using volumetric Boolean intersections. We first tightly bound each input using a pair of oppositely oriented height-fields, forming a Double Height-Field (DHF) Hull. We refine this hull by intersecting it with additional localized height-fields (HFs) that capture surface regions in its interior. We minimize the number of HFs necessary to accurately capture each input and compactly encode both the DHF hull and the local HFs as neural functions defined over subdomains of (mathbb R^2) . This reduced dimensionality encoding delivers high-quality compact approximations. Given similar parameter count, or storage capacity, NESI significantly reduces approximation error compared to the state-of-the-art, especially at lower parameter counts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TOG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/patch_grid.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patch_grid.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="patchgrid2025" class="col-sm-8"> <div class="title">Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface Representation</div> <div class="author"> <a href="https://carrie-lin.github.io/" rel="external nofollow noopener" target="_blank">Guying Lin<sup>*</sup></a>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang<sup>*</sup></a>, <b>Congyi Zhang</b>, <a href="https://haopan.github.io/" rel="external nofollow noopener" target="_blank">Hao Pan</a>, Yuhan Ping, Guodong Wei, <a href="https://i.cs.hku.hk/~taku/" rel="external nofollow noopener" target="_blank">Taku Komura</a>, <a href="https://people.engr.tamu.edu/keyser/index.html" rel="external nofollow noopener" target="_blank">John Keyser</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, Apr 2025 </div> <div class="periodical"> (will be presented at SIGGRAPH 2025, * equal contribution) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3727142" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2308.13934" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:3fE2CSJIrl8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Neural implicit representations are known to be more compact for depicting 3D shapes than traditional discrete representations. However, the neural representations tend to round sharp corners or edges and struggle to represent surfaces with open boundaries. Moreover, they are slow to train. We present a unified neural implicit representation, called Patch-Grid, that fits to complex shapes efficiently, preserves sharp features, and effectively models surfaces with open boundaries and thin geometric features. Our superior efficiency comes from embedding each surface patch into a local latent volume and decoding it using a shared MLP decoder, which is pretrained on various local surface geometries. With this pretrained decoder fixed, fitting novel shapes and local shape updates can be done efficiently. The faithful preservation of sharp features is enabled by adopting a novel merge grid to perform local constructive solid geometry (CSG) combinations of surface patches in the cells of an adaptive Octree, yielding better robustness than using a global CSG construction as proposed in the literature. Experiments show that our Patch-Grid method faithfully captures shapes with complex sharp features, open boundaries and thin structures, and outperforms existing learning-based methods in both efficiency and quality for surface fitting and local shape updates.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">SIGGRAPH 2025</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cmd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cmd.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="CMD2025" class="col-sm-8"> <div class="title">CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</div> <div class="author"> <a href="https://penghtyx.github.io/yuki-lipeng/" rel="external nofollow noopener" target="_blank">Peng Li</a>, Suizhi Ma, Jialiang Chen, <a href="https://liuyuan-pal.github.io/" rel="external nofollow noopener" target="_blank">Yuan Liu<sup>†</sup></a>, <b>Congyi Zhang</b>, Wei Xue, <a href="https://whluo.github.io/" rel="external nofollow noopener" target="_blank">Wenhan Luo<sup>†</sup></a>, <a href="https://www.cs.ubc.ca/~sheffa/" rel="external nofollow noopener" target="_blank">Alla Sheffer</a>, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a>, and <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=yike-guo-yikeguo" rel="external nofollow noopener" target="_blank">Yike Guo</a> </div> <div class="periodical"> <em>In ACM SIGGRAPH 2025 Conference Papers</em>, Vancouver, BC, CA, Aug 2025 </div> <div class="periodical"> († corresponding author) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.07003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/pengHTYX/CMD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://penghtyx.github.io/CMD/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:dhFuZR0502QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TVCG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/lj.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lj.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="LJ2025" class="col-sm-8"> <div class="title">A Potential Field Method for Tooth Motion Planning in Orthodontic Treatment</div> <div class="author"> <a href="https://lym29.github.io/" rel="external nofollow noopener" target="_blank">Yumeng Liu</a>, <a href="https://yuexinma.me/" rel="external nofollow noopener" target="_blank">Yuexin Ma</a>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang</a>, <b>Congyi Zhang<sup>†</sup></b>, <a href="https://gsw-d.github.io/gswei.github.io/" rel="external nofollow noopener" target="_blank">Guangshun Wei</a>, Runnan Chen, <a href="https://facdent.hku.hk/people/professoriate-staff/profile/drgumin" rel="external nofollow noopener" target="_blank">Min Gu</a>, <a href="https://www.cs.hku.hk/people/academic-staff/jpan" rel="external nofollow noopener" target="_blank">Jia Pan</a>, Zhengbao Yang, <a href="https://i.cs.hku.hk/~taku/" rel="external nofollow noopener" target="_blank">Taku Komura</a>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Shiqing Xin, Yuanfeng Zhou, Changhe Tu, Wenping Wang&lt;sup&gt;&lt;sup&gt;†&lt;/sup&gt;&lt;/sup&gt;' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2025 </div> <div class="periodical"> († corresponding author) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2025.3567299" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:L8Ckcad2t8MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Invisible orthodontics, commonly known as clear alignment treatment, offers a more comfortable and aesthetically pleasing alternative in orthodontic care, attracting considerable attention in the dental community in recent years. It replaces conventional metal braces with a series of removable, and transparent aligners. Each aligner is crafted to facilitate a gradual adjustment of the teeth, ensuring progressive stages of dental correction. This necessitates the design for teeth motion. Here we present an automatic method and a system for generating collision-free teeth motion planning while avoiding gaps between adjacent teeth, which is unacceptable in clinical practice. To tackle this task, we formulate it as a constrained optimization problem and utilize the interior point method for its solution. We also developed an interactive system that enables dentists to easily visualize and edit the paths. Our method significantly speeds up the clear aligner planning process, creating the desired motion paths for a full set of teeth in under five minutes—a task that typically requires several hours of manual work. Our experiments and user studies confirm the effectiveness of this method in planning teeth movement, showcasing its potential to streamline orthodontic procedures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">SIGGRAPH 2024</abbr> <figure> <picture> <img src="/assets/img/publication_preview/texpainter.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="texpainter.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TexPainter2024" class="col-sm-8"> <div class="title">TexPainter: Generative Mesh Texturing with Multi-view Consistency</div> <div class="author"> Hongkun Zhang, Zherong Pan, <b>Congyi Zhang</b>, <a href="https://lfzhulf.github.io/" rel="external nofollow noopener" target="_blank">Lifeng Zhu</a>, and <a href="https://gaoxifeng.github.io/" rel="external nofollow noopener" target="_blank">Xifeng Gao</a> </div> <div class="periodical"> <em>In ACM SIGGRAPH 2024 Conference Papers</em>, Denver, CO, USA, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3641519.3657494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2406.18539" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/Quantuman134/TexPainter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://quantuman134.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:aqlVkmm33-oC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The recent success of pre-trained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3D meshes in the wild. However, these models are trained in the screen space, while converting them to a multi-view consistent texture image poses a major obstacle to the output quality. In this paper, we propose a novel method to enforce multi-view consistency. Our method is based on the observation that latent space in a pre-trained diffusion model is noised separately for each camera view, making it difficult to achieve multi-view consistency by directly manipulating the latent codes. Based on the celebrated Denoising Diffusion Implicit Models (DDIM) scheme, we propose to use an optimization-based color-fusion to enforce consistency and indirectly modify the latent codes by gradient back-propagation. Our method further relaxes the sequential dependency assumption among the camera views. By evaluating on a series of general 3D models, we find our simple approach improves consistency and overall quality of the generated textures as compared to competing state-of-the-arts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TVCG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/skull2face.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="skull2face.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Skull2Face2024" class="col-sm-8"> <div class="title">Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing</div> <div class="author"> <a href="https://lyq.me/" rel="external nofollow noopener" target="_blank">Yongqing Liang</a>, <b>Congyi Zhang</b>, Junli Zhao, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a>, and <a href="https://people.tamu.edu/~xinli/" rel="external nofollow noopener" target="_blank">Xin Li</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.16207" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/xmlyqing00/skull-to-face" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://xmlyqing00.github.io/skull-to-face-page/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:M3ejUd6NZC8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Deducing the 3D face from a skull is a challenging task in forensic science and archaeology. This paper proposes an end-to-end 3D face reconstruction pipeline and an exploration method that can conveniently create textured, realistic faces that match the given skull. To this end, we propose a tissue-guided face creation and adaptation scheme. With the help of the state-of-the-art text-to-image diffusion model and parametric face model, we first generate an initial reference 3D face, whose biological profile aligns with the given skull. Then, with the help of tissue thickness distribution, we modify these initial faces to match the skull through a latent optimization process. The joint distribution of tissue thickness is learned on a set of skull landmarks using a collection of scanned skull-face pairs. We also develop an efficient face adaptation tool to allow users to interactively adjust tissue thickness either globally or at local regions to explore different plausible faces. Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TVCG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/pe_sample.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pe_sample.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="PESampling2024" class="col-sm-8"> <div class="title">On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding</div> <div class="author"> <a href="https://carrie-lin.github.io/" rel="external nofollow noopener" target="_blank">Guying Lin<sup>*</sup></a>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang<sup>*</sup></a>, <a href="https://liuyuan-pal.github.io/" rel="external nofollow noopener" target="_blank">Yuan Liu</a>, <b>Congyi Zhang</b>, <a href="https://sites.google.com/site/junhuihoushomepage/" rel="external nofollow noopener" target="_blank">Junhui Hou</a>, <a href="http://www.cad.zju.edu.cn/home/jin/" rel="external nofollow noopener" target="_blank">Xiaogang Jin</a>, <a href="https://i.cs.hku.hk/~taku/" rel="external nofollow noopener" target="_blank">Taku Komura</a>, <a href="https://people.engr.tamu.edu/keyser/index.html" rel="external nofollow noopener" target="_blank">John Keyser</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Dec 2024 </div> <div class="periodical"> (* equal contribution) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2024.3522082" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2401.01391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:_kc_bZDykSQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network’s responses. It is observed that a PE-equipped MLP has an intrinsic frequency much higher than the highest frequency component in the PE layer. Sampling against this intrinsic frequency following the Nyquist-Sannon sampling theorem allows us to determine an appropriate training sampling rate. We empirically show in the setting of SDF fitting that this recommended sampling rate is sufficient to secure accurate fitting results, while further increasing the sampling rate would not further noticeably reduce the fitting error. Training PE-equipped MLPs simply with our sampling strategy leads to performances superior to the existing methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">ICCV 2023</abbr> <figure> <picture> <img src="/assets/img/publication_preview/dualmeshudf.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dualmeshudf.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DualMeshUDF2023" class="col-sm-8"> <div class="title">Surface Extraction from Neural Unsigned Distance Fields</div> <div class="author"> <b>Congyi Zhang<sup>*</sup></b>, <a href="https://carrie-lin.github.io/" rel="external nofollow noopener" target="_blank">Guying Lin<sup>*</sup></a>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang</a>, <a href="https://people.tamu.edu/~xinli/" rel="external nofollow noopener" target="_blank">Xin Li</a>, <a href="https://i.cs.hku.hk/~taku/" rel="external nofollow noopener" target="_blank">Taku Komura</a>, <a href="https://people.engr.tamu.edu/schaefer/index.html" rel="external nofollow noopener" target="_blank">Scott Schaefer</a>, <a href="https://people.engr.tamu.edu/keyser/index.html" rel="external nofollow noopener" target="_blank">John Keyser</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, Oct 2023 </div> <div class="periodical"> (* equal contribution) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/cong-yi/DualMesh-UDF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cong-yi.github.io/projects/dualmeshudf/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:Zph67rFs4hoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We propose a method, named DualMesh-UDF, to extract a surface from unsigned distance functions (UDFs), encoded by neural networks, or neural UDFs. Neural UDFs are becoming increasingly popular for surface representation because of their versatility in presenting surfaces with arbitrary topologies, as opposed to the signed distance function that is limited to representing a closed surface. However, the applications of neural UDFs are hindered by the notorious difficulty in extracting the target surfaces they represent. Recent methods for surface extraction from a neural UDF suffer from significant geometric errors or topological artifacts due to two main difficulties: (1) A UDF does not exhibit sign changes; and (2) A neural UDF typically has substantial approximation errors. DualMesh-UDF addresses these two difficulties. Specifically, given a neural UDF encoding a target surface S to be recovered, we first estimate the tangent planes of S at a set of sample points close to S. Next, we organize these sample points into local clusters, and for each local cluster, solve a linear least squares problem to determine a final surface point. These surface points are then connected to create the output mesh surface, which approximates the target surface. The robust estimation of the tangent planes of the target surface and the subsequent minimization problem constitute our core strategy, which contributes to the favorable performance of DualMesh-UDF over other competing methods. To efficiently implement this strategy, we employ an adaptive Octree. Within this framework, we estimate the location of a surface point in each of the octree cells identified as containing part of the target surface. Extensive experiments show that our method outperforms existing methods in terms of surface reconstruction quality while maintaining comparable computational efficiency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">ICCV 2023</abbr> <figure> <picture> <img src="/assets/img/publication_preview/sherd.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sherd.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Sherd2023" class="col-sm-8"> <div class="title">Batch-based Model Registration for Fast 3D Sherd Reconstruction</div> <div class="author"> <a href="https://jiepengwang.github.io/" rel="external nofollow noopener" target="_blank">Jiepeng Wang</a>, <b>Congyi Zhang</b>, <a href="https://totoro97.github.io/" rel="external nofollow noopener" target="_blank">Peng Wang</a>, <a href="https://people.tamu.edu/~xinli/" rel="external nofollow noopener" target="_blank">Xin Li</a>, Peter J. Cobb, <a href="https://people.mpi-inf.mpg.de/~theobalt/" rel="external nofollow noopener" target="_blank">Christian Theobalt</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.06897" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jiepengwang/FIRES" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://jiepengwang.github.io/FIRES/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:YOwf2qJgpHMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>3D reconstruction techniques have widely been used for digital documentation of archaeological fragments. However, efficient digital capture of fragments remains as a challenge. In this work, we aim to develop a portable, high-throughput, and accurate reconstruction system for efficient digitization of fragments excavated in archaeological sites. To realize high-throughput digitization of large numbers of objects, an effective strategy is to perform scanning and reconstruction in batches. However, effective batch-based scanning and reconstruction face two key challenges: 1) how to correlate partial scans of the same object from multiple batch scans, and 2) how to register and reconstruct complete models from partial scans that exhibit only small overlaps. To tackle these two challenges, we develop a new batch-based matching algorithm that pairs the front and back sides of the fragments, and a new Bilateral Boundary ICP algorithm that can register partial scans sharing very narrow overlapping regions. Extensive validation in labs and testing in excavation sites demonstrate that these designs enable efficient batch-based scanning for fragments. We show that such a batch-based scanning and reconstruction pipeline can have immediate applications on digitizing sherds in archaeological excavations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">SIGGRAPH ASIA 2022</abbr> <figure> <picture> <img src="/assets/img/publication_preview/dmm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dmm.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DMM2022" class="col-sm-8"> <div class="title">An Implicit Parametric Morphable Dental Model</div> <div class="author"> <b>Congyi Zhang</b>, <a href="https://people.mpi-inf.mpg.de/~elgharib/" rel="external nofollow noopener" target="_blank">Mohamed Elgharib</a>, <a href="https://people.mpi-inf.mpg.de/~gfox/" rel="external nofollow noopener" target="_blank">Gereon Fox</a>, <a href="https://facdent.hku.hk/people/professoriate-staff/profile/drgumin" rel="external nofollow noopener" target="_blank">Min Gu</a>, <a href="https://people.mpi-inf.mpg.de/~theobalt/" rel="external nofollow noopener" target="_blank">Christian Theobalt</a>, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3550454.3555469" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2211.11402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/cong-yi/DMM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vcai.mpi-inf.mpg.de/projects/DMM/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>3D Morphable models of the human body capture variations among subjects and are useful in reconstruction and editing applications. Current dental models use an explicit mesh scene representation and model only the teeth, ignoring the gum. In this work, we present the first parametric 3D morphable dental model for both teeth and gum. Our model uses an implicit scene representation and is learned from rigidly aligned scans. It is based on a component-wise representation for each tooth and the gum, together with a learnable latent code for each of such components. It also learns a template shape thus enabling several applications such as segmentation, interpolation and tooth replacement. Our reconstruction quality is on par with the most advanced global implicit representations while enabling novel applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">TVCG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/creatureshop.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="creatureshop.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="CreatureShop2022" class="col-sm-8"> <div class="title">CreatureShop: Interactive 3D Character Modeling and Texturing from a Single Color Drawing</div> <div class="author"> <b>Congyi Zhang</b>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang</a>, Nenglun Chen, <a href="https://www.cs.ubc.ca/~nvining/" rel="external nofollow noopener" target="_blank">Nicholas Vining</a>, <a href="https://www.cs.ubc.ca/~sheffa/" rel="external nofollow noopener" target="_blank">Alla Sheffer</a>, <a href="https://i.cs.hku.hk/~fcmlau/" rel="external nofollow noopener" target="_blank">Francis C.M. Lau</a>, Guoping Wang, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.05572" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://cong-yi.github.io/projects/creatureshop/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Creating 3D shapes from 2D drawings is an important problem with applications in content creation for computer animation and virtual reality. We introduce a new sketch-based system, CreatureShop, that enables amateurs to create high-quality textured 3D character models from 2D drawings with ease and efficiency. CreatureShop takes an input bitmap drawing of a character (such as an animal or other creature), depicted from an arbitrary descriptive pose and viewpoint, and creates a 3D shape with plausible geometric details and textures from a small number of user annotations on the 2D drawing. Our key contributions are a novel oblique view modeling method, a set of systematic approaches for producing plausible textures on the invisible or occluded parts of the 3D character (as viewed from the direction of the input drawing), and a user-friendly interactive system. We validate our system and methods by creating numerous 3D characters from various drawings, and compare our results with related works to show the advantages of our method. We perform a user study to evaluate the usability of our system, which demonstrates that our system is a practical and efficient approach to create fully-textured 3D character models for novice users.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">CHI 2021</abbr> <figure> <picture> <img src="/assets/img/publication_preview/handpainter.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="handpainter.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HandPainter2021" class="col-sm-8"> <div class="title">HandPainter - 3D Sketching in VR with Hand-Based Physical Proxy</div> <div class="author"> <a href="https://yingjiang96.github.io/" rel="external nofollow noopener" target="_blank">Ying Jiang</a>, <b>Congyi Zhang<sup>†</sup></b>, <a href="http://sweb.cityu.edu.hk/hongbofu/" rel="external nofollow noopener" target="_blank">Hongbo Fu</a>, <a href="https://scholar.google.it/citations?user=P-3NxFMAAAAJ&amp;hl=it" rel="external nofollow noopener" target="_blank">Alberto Cannavò</a>, <a href="https://staff.polito.it/fabrizio.lamberti/" rel="external nofollow noopener" target="_blank">Fabrizio Lamberti</a>, Henry Y K Lau, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, Yokohama, Japan, Dec 2021 </div> <div class="periodical"> († corresponding author) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3411764.3445302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://yingjiang96.github.io/handpainter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index finger of the other hand as a 3D pen. To this end, we first perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the findings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the effectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">IEEE VR 2020</abbr> <figure> <picture> <img src="/assets/img/publication_preview/dgaze.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dgaze.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="DGaze2020" class="col-sm-8"> <div class="title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</div> <div class="author"> <a href="https://cranehzm.github.io/" rel="external nofollow noopener" target="_blank">Zhiming Hu</a>, Sheng Li, <b>Congyi Zhang</b>, Kangrui Yi, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2020.2973473" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://cranehzm.github.io/DGaze.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We conduct novel analyses of users’ gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. We first collect 43 users’ eye tracking data in 5 dynamic scenes under free-viewing conditions. Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users’ gaze positions. Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users’ gaze positions. Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. We further analyze our CNN architecture and verify the effectiveness of each component in our model. We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">SIGGRAPH ASIA 2020</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cppm.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cppm.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="CPPM2020" class="col-sm-8"> <div class="title">CPPM: Chi-Squared Progressive Photon Mapping</div> <div class="author"> Zehui Lin, Sheng Li, Xinlu Zeng, <b>Congyi Zhang</b>, Jinzhu Jia, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>ACM Trans. Graph.</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3414685.3417822" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://bactlink.github.io/CPPM.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We present a novel chi-squared progressive photon mapping algorithm (CPPM) that constructs an estimator by controlling the bandwidth to obtain superior image quality. Our estimator has parametric statistical advantages over prior nonparametric methods. First, we show that when a probability density function of the photon distribution is subject to uniform distribution, the radiance estimation is unbiased under certain assumptions. Next, the local photon distribution is evaluated via a chi-squared test to determine whether the photons follow the hypothesized distribution (uniform distribution) or not. If the statistical test deems that the photons inside the bandwidth are uniformly distributed, bandwidth reduction should be suspended. Finally, we present a pipeline with a bandwidth retention and conditional reduction scheme according to the test results. This pipeline not only accumulates sufficient photons for a reliable chi-squared test, but also guarantees that the estimate converges to the correct solution under our assumptions. We evaluate our method on various benchmarks and observe significant improvement in the running time and rendering quality in terms of mean squared error over prior progressive photon mapping methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">SMI 2019</abbr> <figure> <picture> <img src="/assets/img/publication_preview/deform.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deform.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SemanticDeform2019" class="col-sm-8"> <div class="title">Real-time editing of man-made mesh models under geometric constraints</div> <div class="author"> <b>Congyi Zhang</b>, <a href="https://leiyangjustin.github.io/" rel="external nofollow noopener" target="_blank">Lei Yang</a>, Liyou Xu, Guoping Wang, and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" rel="external nofollow noopener" target="_blank">Wenping Wang</a> </div> <div class="periodical"> <em>Computers &amp; Graphics</em>, Nov 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.cag.2019.05.028" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Editing man-made mesh models under multiple geometric constraints is a crucial need for product design to facilitate design exploration and iterative optimization. However, the presence of multiple geometric constraints (e.g. the radius of a cylindrical shape, distance from a point to a plane) as well as the high dimensionality of the discrete mesh representation of man-made models make it difficult to solve this constraint system in real-time. In this paper, we propose an approach based on subspace decomposition to achieve this goal. When a set of variables are edited by the user, the proposed method minimizes the residual of the constraint system in a least square sense to derive a new shape. The resulting shape shall comply with the assigned (extrinsic) constraints while maintaining the original (intrinsic) constraints analyzed from the given mesh model. In particular, we extract a meaningful subspace of the entire solution space based on the user’s edits to reduce the order of the problem, and solve the constraint system globally in real-time. Finally, we project the approximate solution back to the original solution space to obtain the editing result.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge rounded w-100">IEEE VR 2019</abbr> <figure> <picture> <img src="/assets/img/publication_preview/sgaze.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sgaze.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SGaze2019" class="col-sm-8"> <div class="title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</div> <div class="author"> <a href="https://cranehzm.github.io/" rel="external nofollow noopener" target="_blank">Zhiming Hu</a>, <b>Congyi Zhang</b>, Sheng Li, Guoping Wang, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2019.2899187" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://cranehzm.github.io/SGaze.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-Ke6AGEAAAAJ&amp;citation_for_view=-Ke6AGEAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://dblp.org/pid/192/2791.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="mailto:%63%6F%6E%67%79%69%7A@%63%73.%75%62%63.%63%61" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/cong-yi" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/congyi-zhang-391790135" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=-Ke6AGEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Congyi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0D6W2H20J1"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0D6W2H20J1');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>